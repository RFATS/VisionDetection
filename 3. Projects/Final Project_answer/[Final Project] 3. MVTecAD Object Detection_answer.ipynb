{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPrIi6lz0pxuu54sPUENSzQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"HyCiqD8RZbEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HReK50wYR2W"},"outputs":[],"source":["# ==================================================\n","# 0. 의존성\n","# ==================================================\n","import os, json, random\n","from glob import glob\n","from collections import defaultdict\n","from typing import List, Tuple, Dict\n","\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms.functional as F\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import torch.optim as optim\n","from tqdm.auto import tqdm\n","\n","# ==================================================\n","# 1. 전역 설정\n","# ==================================================\n","ROOT_DIR    = \"/content/drive/MyDrive/Data/MVTecAD\"\n","BATCH_SIZE  = 2\n","NUM_WORKERS = 4\n","LR = 1e-4\n","EPOCHS = 50\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)\n","\n"]},{"cell_type":"code","source":["# ==================================================\n","# 2. COCO JSON 파싱 → {file_name: [[x1,y1,x2,y2], ...]}\n","# ==================================================\n","def parse_coco_json(json_path: str) -> Dict[str, List[List[float]]]:\n","    with open(json_path) as f:\n","        coco = json.load(f)\n","\n","    id_to_name = {img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]}\n","\n","    fname2boxes = defaultdict(list)\n","    for ann in coco[\"annotations\"]:\n","        x, y, w, h = ann[\"bbox\"]\n","        box = [x, y, x + w, y + h]\n","        fname2boxes[id_to_name[ann[\"image_id\"]]].append(box)\n","\n","    return fname2boxes\n","\n","# ==================================================\n","# 3. Dataset\n","# ==================================================\n","class MVTecODDataset(Dataset):\n","    \"\"\"\n","    test/<defect>/ 이미지 ↔ ground_truth/<defect>/COCO-json(1개)\n","    \"\"\"\n","    def __init__(self, root_dir: str, transforms=None, use_good: bool=False):\n","        self.root_dir   = root_dir\n","        self.transforms = transforms\n","        self.samples: List[Tuple[str, List[List[float]], int]] = []\n","        self.label_map: Dict[str, int] = {}\n","        lbl_idx = 1   # 0 = background\n","\n","        for category in sorted(os.listdir(root_dir)):\n","            cat_dir = os.path.join(root_dir, category)\n","            if not os.path.isdir(cat_dir):\n","                continue\n","\n","            test_dir = os.path.join(cat_dir, \"test\")\n","            gt_dir   = os.path.join(cat_dir, \"ground_truth\")\n","            if not (os.path.isdir(test_dir) and os.path.isdir(gt_dir)):\n","                continue\n","\n","            for defect in sorted(os.listdir(test_dir)):\n","                if defect == \"good\" and not use_good:\n","                    continue\n","\n","                img_dir  = os.path.join(test_dir, defect)\n","                ann_json = glob(os.path.join(gt_dir, defect, \"*.json\"))\n","                if len(ann_json) != 1:\n","                    print(f\"[경고] {category}/{defect} JSON 수={len(ann_json)} → 스킵\")\n","                    continue\n","\n","                # label id 부여\n","                if defect not in self.label_map:\n","                    self.label_map[defect] = lbl_idx\n","                    lbl_idx += 1\n","                label_id = self.label_map[defect]\n","\n","                # COCO JSON → fname➜boxes\n","                fname2boxes = parse_coco_json(ann_json[0])\n","\n","                for fname, boxes in fname2boxes.items():\n","                    # --- 원본 이미지 경로 찾기 -----------------------------\n","                    base = fname.replace(\"_mask\", \"\")\n","                    cand = [os.path.join(img_dir, base),\n","                            os.path.join(img_dir, base[:-4] + \".png\"),\n","                            os.path.join(img_dir, base[:-4] + \".jpg\"),\n","                            os.path.join(img_dir, fname)]     # 마스크 자체\n","                    img_path = next((p for p in cand if os.path.isfile(p)), None)\n","                    if img_path is None:\n","                        continue\n","                    # ------------------------------------------------------\n","                    self.samples.append((img_path, boxes, label_id))\n","\n","        print(f\"총 {len(self.samples)}개 이미지,  라벨 맵: {self.label_map}\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, boxes, label_id = self.samples[idx]\n","\n","        img    = Image.open(img_path).convert(\"RGB\")\n","        boxes  = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.full((len(boxes),), label_id, dtype=torch.int64)\n","\n","        target = {\"boxes\": boxes, \"labels\": labels}\n","\n","        if self.transforms:\n","            img = self.transforms(img)\n","\n","        return img, target, img_path   # ← 경로도 함께 반환\n"],"metadata":{"id":"Fa-EoSdacYxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================================================\n","# 4. Dataset 인스턴스 생성\n","# ==================================================\n","dataset = MVTecODDataset(\n","    ROOT_DIR,\n","    transforms=lambda x: F.to_tensor(x),\n","    use_good=False\n",")\n","\n","# ==================================================\n","# 5. train / val split ─ random_split\n","# ==================================================\n","train_ratio = 0.8\n","n_total     = len(dataset)\n","n_train     = int(train_ratio * n_total)\n","n_val       = n_total - n_train\n","\n","train_set, val_set = torch.utils.data.random_split(\n","    dataset,\n","    [n_train, n_val],\n","    generator=torch.Generator()   # 재현 가능\n",")\n","\n","print(f\"train: {len(train_set)} / val: {len(val_set)}\")\n","\n","# ==================================================\n","# 6. DataLoader 설정\n","# ==================================================\n","def collate_fn(batch):\n","    imgs, targets, paths = zip(*batch)\n","    return list(imgs), list(targets), list(paths)\n","\n","train_loader = DataLoader(\n","    train_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=NUM_WORKERS,\n","    collate_fn=collate_fn,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,          # 검증은 셔플 X\n","    num_workers=NUM_WORKERS,\n","    collate_fn=collate_fn,\n","    pin_memory=True\n",")\n"],"metadata":{"id":"5iefr3guZV8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------------------------------------\n","# 7. 간단 테스트 루프\n","# --------------------------------------------------\n","for phase, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n","    imgs, targets, paths = next(iter(loader))\n","    print(f\"[{phase}]  첫 경로:\", paths[0])\n","    print(f\"[{phase}]  첫 bbox:\", targets[0][\"boxes\"][0])"],"metadata":{"id":"G5t9jIcVeoCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================================================\n","# 8. Object Detection 모델 구현 및 학습 스크립트\n","# ==================================================\n","# TODO"],"metadata":{"id":"_TbkzQs1bUwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================================================\n","# 9. 시각화: 예측 ↔ GT 비교\n","# ==================================================\n","import matplotlib.pyplot as plt\n","from torchvision.utils import draw_bounding_boxes\n","import torchvision.transforms.functional as TF\n","\n","# id → 이름 역매핑  (→ draw_bounding_boxes 의 labels 인자에 사용)\n","id2name = {v: k for k, v in dataset.label_map.items()}\n","\n","def vis_predictions(model, loader, device,\n","                    score_thresh: float = 0.5,\n","                    num_images: int = 4):\n","    \"\"\"\n","    • 검증 배치에서 num_images 장을 골라\n","      └ 왼쪽 : Ground-Truth (GREEN)\n","      └ 오른쪽: Prediction (RED, score ≥ thresh)\n","    \"\"\"\n","    model.eval()\n","    shown = 0\n","    plt.figure(figsize=(num_images * 4, 4))\n","\n","    with torch.no_grad():\n","        for imgs, targets, paths in loader:\n","            imgs = [img.to(device) for img in imgs]\n","            outputs = model(imgs)\n","\n","            for img, tgt, pred in zip(imgs, targets, outputs):\n","                # ----------------- GT 그리기 -----------------\n","                gt_boxes  = tgt[\"boxes\"].cpu()\n","                gt_labels = [id2name[int(l)] for l in tgt[\"labels\"]]\n","                gt_img = draw_bounding_boxes(\n","                    (img.cpu() * 255).byte(),\n","                    gt_boxes,\n","                    labels=gt_labels,\n","                    colors=\"green\",\n","                    width=16,          # ← 굵기 UP (기본 1)\n","                    font_size=18      # ← 글자 크기 UP (기본 10)\n","                )\n","                # ----------------- Pred 그리기 ----------------\n","                keep = pred[\"scores\"] >= score_thresh\n","                pred_boxes  = pred[\"boxes\"][keep].cpu()\n","                pred_labels = [id2name[int(l)]\n","                               for l in pred[\"labels\"][keep]]\n","                pred_scores = pred[\"scores\"][keep].cpu().tolist()\n","                # 라벨 뒤에 점수 표기\n","                pred_labels = [f\"{lab}:{s:.2f}\"\n","                               for lab, s in zip(pred_labels, pred_scores)]\n","\n","                pred_img = draw_bounding_boxes(\n","                    (img.cpu() * 255).byte(),\n","                    pred_boxes,\n","                    labels=pred_labels,\n","                    colors=\"red\",\n","                    width=16,          # ← 동일하게 굵기·글자 크기 조정\n","                    font_size=18\n","                )\n","\n","                # ----------------- 시각화 ---------------------\n","                gt_np   = TF.to_pil_image(gt_img)\n","                pred_np = TF.to_pil_image(pred_img)\n","\n","                plt.subplot(2, num_images, shown + 1)\n","                plt.imshow(gt_np);   plt.axis(\"off\")\n","                if shown == 0: plt.title(\"Ground-Truth\", fontsize=12)\n","\n","                plt.subplot(2, num_images, num_images + shown + 1)\n","                plt.imshow(pred_np); plt.axis(\"off\")\n","                if shown == 0: plt.title(\"Prediction\", fontsize=12)\n","\n","                shown += 1\n","                if shown >= num_images:\n","                    plt.tight_layout()\n","                    plt.show()\n","                    return\n"],"metadata":{"id":"9pVf64Ynfg-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================================================\n","# 10. 시각화\n","# ==================================================\n","vis_predictions(model, val_loader, device,\n","                score_thresh=0.5, num_images=4)\n"],"metadata":{"id":"_bwopqQCor0L"},"execution_count":null,"outputs":[]}]}